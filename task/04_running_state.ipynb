{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dace1a29-864a-4a69-bd00-c0952a73d4ea",
   "metadata": {
    "id": "dace1a29-864a-4a69-bd00-c0952a73d4ea"
   },
   "source": [
    "<center><a href=\"https://www.nvidia.com/en-us/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TONXsdAOC5Dn",
   "metadata": {
    "id": "TONXsdAOC5Dn"
   },
   "source": [
    "<br>\n",
    "\n",
    "# <font color=\"#76b900\">**Notebook 4:** Running State Chains</font>\n",
    "\n",
    "<br>\n",
    "\n",
    "In the previous notebook, we introduced some key LangChain Expression Language (LCEL) material regarding runnables. By now, you should be comfortable with both internal and external reasoning, as well as how to develop pipelines that facilitate it! In this notebook, we will make our way towards more advanced paradigms that will allow us to orchestrate more complex dialog management strategies and begin to execute on longer-form document reasoning.\n",
    "\n",
    "<br>\n",
    "\n",
    "### **Learning Objectives:**\n",
    "\n",
    "- Learning how to leverage runnables to orchestrate interesting LLM systems.  \n",
    "- Understanding how running state chains can be used for dialog management and iterative decision-making.\n",
    "\n",
    "<br>\n",
    "\n",
    "### **Questions To Think About:**\n",
    "\n",
    "- Would there ever be a use for a single-module variant of the running state chain that is not constantly querying the environment for input?\n",
    "- You may notice that the JSON prediction is actually working pretty well. It might not always work so well depending on the questions and the JSON format complexity. What kinds of issues do you expect to encounter in this regard?\n",
    "- What kinds of approaches can you think of completely swapping prompts as part of the running state chain?\n",
    "\n",
    "<br>\n",
    "\n",
    "### **Notebook Source:**\n",
    "\n",
    "- This notebook is part of a larger [**NVIDIA Deep Learning Institute**](https://www.nvidia.com/en-us/training/) course titled [**Building RAG Agents with LLMs**](https://www.nvidia.com/en-sg/training/instructor-led-workshops/building-rag-agents-with-llms/). If sharing this material, please give credit and link back to the original course.\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "### **Environment Setup:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5AbHxz61Hq9x",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 44079,
     "status": "ok",
     "timestamp": 1703083363284,
     "user": {
      "displayName": "Vadim Kudlay",
      "userId": "00553664172613290122"
     },
     "user_tz": 360
    },
    "id": "5AbHxz61Hq9x",
    "outputId": "b50f4635-f16e-44f9-da54-fdf108c83445"
   },
   "outputs": [],
   "source": [
    "## Necessary for Colab, not necessary for course environment\n",
    "# %pip install -q langchain langchain-nvidia-ai-endpoints gradio\n",
    "\n",
    "# import os\n",
    "# os.environ[\"NVIDIA_API_KEY\"] = \"nvapi-...\"\n",
    "\n",
    "from functools import partial\n",
    "from rich.console import Console\n",
    "from rich.style import Style\n",
    "from rich.theme import Theme\n",
    "\n",
    "console = Console()\n",
    "base_style = Style(color=\"#76B900\", bold=True)\n",
    "pprint = partial(console.print, style=base_style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d8c16f-78af-4ea3-b1f8-e2caff0fe733",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "ChatNVIDIA.get_available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce88bae-906b-49fd-9209-141c649d5320",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Useful utility method for printing intermediate states\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from functools import partial\n",
    "\n",
    "def RPrint(preface=\"State: \"):\n",
    "    def print_and_return(x, preface=\"\"):\n",
    "        print(f\"{preface}{x}\")\n",
    "        return x\n",
    "    return RunnableLambda(partial(print_and_return, preface=preface))\n",
    "\n",
    "def PPrint(preface=\"State: \"):\n",
    "    def print_and_return(x, preface=\"\"):\n",
    "        pprint(preface, x)\n",
    "        return x\n",
    "    return RunnableLambda(partial(print_and_return, preface=preface))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf2e4ef-6e3a-4796-b51e-c9da0c156ded",
   "metadata": {
    "id": "0bf2e4ef-6e3a-4796-b51e-c9da0c156ded"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 1:** Keeping Variables Flowing\n",
    "\n",
    "In the previous examples, we were able to implement interesting logic in our standalone chains by **creating**, **mutating**, and **consuming** states. These states were passed around as dictionaries with descriptive keys and useful values, and the values would be used to supply follow-up routines with the info they need to operate!\n",
    "\n",
    "**Recall the zero-shot classification example from the last notebook:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa215f6-050c-4062-a455-feffd77daaeb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5829,
     "status": "ok",
     "timestamp": 1703083414406,
     "user": {
      "displayName": "Vadim Kudlay",
      "userId": "00553664172613290122"
     },
     "user_tz": 360
    },
    "id": "7aa215f6-050c-4062-a455-feffd77daaeb",
    "outputId": "df76d362-e2b9-437c-910f-aa1127cd3f44"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "## ^^ This notebook is timed, which will print out how long it all took\n",
    "\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from typing import List, Union\n",
    "from operator import itemgetter\n",
    "\n",
    "## Zero-shot classification prompt and chain w/ explicit few-shot prompting\n",
    "sys_msg = (\n",
    "    \"Choose the most likely topic classification given the sentence as context.\"\n",
    "    \" Only one word, no explanation.\\n[Options : {options}]\"\n",
    ")\n",
    "\n",
    "zsc_prompt = ChatPromptTemplate.from_template(\n",
    "    f\"{sys_msg}\\n\\n\"\n",
    "    \"[[The sea is awesome]][/INST]boat</s><s>[INST]\"\n",
    "    \"[[{input}]]\"\n",
    ")\n",
    "\n",
    "## Define your simple instruct_model\n",
    "instruct_chat = ChatNVIDIA(model=\"mistralai/mistral-7b-instruct-v0.2\")\n",
    "instruct_llm = instruct_chat | StrOutputParser()\n",
    "one_word_llm = instruct_chat.bind(stop=[\" \", \"\\n\"]) | StrOutputParser()\n",
    "\n",
    "zsc_chain = zsc_prompt | one_word_llm\n",
    "\n",
    "## Function that just prints out the first word of the output. With early stopping bind\n",
    "def zsc_call(input, options=[\"car\", \"boat\", \"airplane\", \"bike\"]):\n",
    "    return zsc_chain.invoke({\"input\" : input, \"options\" : options}).split()[0]\n",
    "\n",
    "print(\"-\" * 80)\n",
    "print(zsc_call(\"Should I take the next exit, or keep going to the next one?\"))\n",
    "\n",
    "print(\"-\" * 80)\n",
    "print(zsc_call(\"I get seasick, so I think I'll pass on the trip\"))\n",
    "\n",
    "print(\"-\" * 80)\n",
    "print(zsc_call(\"I'm scared of heights, so flying probably isn't for me\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4HYI7SIxLs5W",
   "metadata": {
    "id": "4HYI7SIxLs5W"
   },
   "source": [
    "<br>\n",
    "\n",
    "This chain makes several design decisions that make it very easy to use, key among them the following:\n",
    "\n",
    "**We want it to act like a function, so all we want it to do is generate the output and return it.**\n",
    "\n",
    "This makes the chain extremely natural for inclusion as a module in a larger chain system. For example, the following chain will take a string, extract the most likely topic, and then generate a new sentence based on the topic:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LuldQwa_PQNS",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "executionInfo": {
     "elapsed": 3827,
     "status": "ok",
     "timestamp": 1703083418214,
     "user": {
      "displayName": "Vadim Kudlay",
      "userId": "00553664172613290122"
     },
     "user_tz": 360
    },
    "id": "LuldQwa_PQNS",
    "outputId": "cdca3240-8037-4b31-e336-02a271017b6b"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "## ^^ This notebook is timed, which will print out how long it all took\n",
    "gen_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Make a new sentence about the the following topic: {topic}. Be creative!\"\n",
    ")\n",
    "\n",
    "gen_chain = gen_prompt | instruct_llm\n",
    "\n",
    "input_msg = \"I get seasick, so I think I'll pass on the trip\"\n",
    "options = [\"car\", \"boat\", \"airplane\", \"bike\"]\n",
    "\n",
    "chain = (\n",
    "    ## -> {\"input\", \"options\"}\n",
    "    {'topic' : zsc_chain}\n",
    "    | PPrint()\n",
    "    ## -> {**, \"topic\"}\n",
    "    | gen_chain\n",
    "    ## -> string\n",
    ")\n",
    "\n",
    "chain.invoke({\"input\" : input_msg, \"options\" : options})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1zzDhPsfXRhn",
   "metadata": {
    "id": "1zzDhPsfXRhn"
   },
   "source": [
    "<br>\n",
    "\n",
    "However, it's a bit problematic when you want to keep the information flowing, since we lose the topic and input variables in generating our response. If we wanted to do something with both the output and the input, we'd need a way to make sure that both variables pass through.\n",
    "\n",
    "Lucky for us, we can use the mapping runnable (i.e. interpretted from a dictionary or using manual `RunnableMap`) to pass both of the variables through by assigning the output of our chain to just a single key and letting the other keys propagate as desired. Alternatively, we could also use `RunnableAssign` to merge the state-consuming chain's output with the input dictionary by default.\n",
    "\n",
    "With this technique, we can propagate whatever we want through our chain system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MfXm7oT5XVOd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1703083424179,
     "user": {
      "displayName": "Vadim Kudlay",
      "userId": "00553664172613290122"
     },
     "user_tz": 360
    },
    "id": "MfXm7oT5XVOd",
    "outputId": "895b237d-dd15-4c7c-8d0e-b25be664e4a3"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "## ^^ This notebook is timed, which will print out how long it all took\n",
    "\n",
    "from langchain.schema.runnable import RunnableBranch, RunnablePassthrough\n",
    "from langchain.schema.runnable.passthrough import RunnableAssign\n",
    "from functools import partial\n",
    "\n",
    "big_chain = (\n",
    "    PPrint()\n",
    "    ## Manual mapping. Can be useful sometimes and inside branch chains\n",
    "    | {'input' : lambda d: d.get('input'), 'topic' : zsc_chain}\n",
    "    | PPrint()\n",
    "    ## RunnableAssign passing. Better for running state chains by default\n",
    "    | RunnableAssign({'generation' : gen_chain})\n",
    "    | PPrint()\n",
    "    ## Using the input and generation together\n",
    "    | RunnableAssign({'combination' : (\n",
    "        ChatPromptTemplate.from_template(\n",
    "            \"Consider the following passages:\"\n",
    "            \"\\nP1: {input}\"\n",
    "            \"\\nP2: {generation}\"\n",
    "            \"\\n\\nCombine the ideas from both sentences into one simple one.\"\n",
    "        )\n",
    "        | instruct_llm\n",
    "    )})\n",
    ")\n",
    "\n",
    "output = big_chain.invoke({\n",
    "    \"input\" : \"I get seasick, so I think I'll pass on the trip\",\n",
    "    \"options\" : [\"car\", \"boat\", \"airplane\", \"bike\", \"unknown\"]\n",
    "})\n",
    "pprint(\"Final Output: \", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tHONiEtGK2qv",
   "metadata": {
    "id": "tHONiEtGK2qv"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 2:** Running State Chain\n",
    "\n",
    "The example above is just a toy example and, if anything, showcases the drawbacks of chaining many LLM calls together for internal under-the-hood reasoning. However, the ability to keep information flowing through a chain is invaluable for making complex chains that can accumulate useful state information or operate in a multi-pass capacity.\n",
    "\n",
    "Specifically, a very simple but effective chain is a **Running State Chain** which enforces the following properties:\n",
    "- A **\"running state\"** is a dictionary that contains all of the variables that the system cares about.\n",
    "- A **\"branch\"** is a chain that can pull in the running state and can degenerate it into a response.\n",
    "- A **branch** can only be ran inside a **RunnableAssign** scope, and the branchs' inputs should come from the **running state**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nGOBMbF-phA6",
   "metadata": {
    "id": "nGOBMbF-phA6"
   },
   "source": [
    "> <img src=\"https://dli-lms.s3.amazonaws.com/assets/s-fx-15-v1/imgs/running_state_chain.png\" width=1000px/>\n",
    "<!-- > <img src=\"https://drive.google.com/uc?export=view&id=1Oo7AauYGj4dxepNReRG2JezmvQLyqXsN\" width=1000px/> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70M-ZOpMpinO",
   "metadata": {
    "id": "70M-ZOpMpinO"
   },
   "source": [
    "You can think of the running state chain abstraction as a functional variant of a Pythonic class with state variables (or attributes) and functions (or methods).\n",
    "- The chain is like the abstract class that wraps all of the functionality.\n",
    "- The running state are like the attributes (which should always be accessible).\n",
    "- The branches are like the class methods (which can pick and choose which attributes to use).\n",
    "- The `.invoke` or similar process is like the `__call__` method that runs through the branches in order.\n",
    "\n",
    "**By forcing this paradigm in your chains:**\n",
    "- You can keep state variables propagating through your chain, allowing your internals to access whatever is necessary and accumulating state values for use later.\n",
    "- You can also pass the outputs of your chain back through as your inputs, allowing a \"while-loop\"-style chain that keeps updating and building on your running state.\n",
    "\n",
    "The rest of this notebook will include two exercises that flesh out the running state chain abstraction for two additional use-cases: **Knowledge Bases** and **Database-Querying Chatbots**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "--3BDo7-wFBF",
   "metadata": {
    "id": "--3BDo7-wFBF"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 3:** Implementing a Knowledge Base with Running State Chain\n",
    "\n",
    "After understanding the basic structure and principles of a Running State Chain, we can explore how this approach can be extended to manage more complex tasks, particularly in creating dynamic systems that evolve through interaction. This section will focus on implementing a **knowledge base** accumulated using **json-enabled slot filling**:\n",
    "\n",
    "- **Knowledge Base:** A store of information that's relevant for our LLM to keep track of.\n",
    "- **JSON-Enabled Slot Filling:** The technique of asking an instruction-tuned model to output a json-style format (which can include a dictionary) with a selection of slots, relying on the LLM to fill these slots with useful and relevant information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imHxfz52wSgY",
   "metadata": {
    "id": "imHxfz52wSgY"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### **Defining Our Knowledge Base**\n",
    "\n",
    "To build a responsive and intelligent system, we need a method that not only processes inputs but also retains and updates essential information through the flow of conversation. This is where the combination of LangChain and Pydantic becomes pivotal. [**Pydantic**](https://docs.pydantic.dev/latest/), a popular Python validation library, is instrumental in structuring and validating data models. As one of its features, Pydantic offers structured \"model\" classes that validate objects (data, classes, themselves, etc.) with simplified syntax and deep rabbitholes of customization options. This framework is used throughout LangChain and comes up as a necessary component for use cases that involve data coersion.\n",
    "\n",
    "One thing that a \"model\" is very good for is defining a class with expected arguments and some special ways to validate them! In this course, we won't focus too much on the validation scripts, but those interested can start by checking out the [**Pydantic Validator guide**](https://docs.pydantic.dev/1.10/usage/validators/) (though the topics do get pretty deep pretty fast). For our purposes, we can construct a `BaseModel` class and define some `Field` variables to create a structured **Knowledge Base** like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WWaYkSP9wQaF",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1703083424179,
     "user": {
      "displayName": "Vadim Kudlay",
      "userId": "00553664172613290122"
     },
     "user_tz": 360
    },
    "id": "WWaYkSP9wQaF",
    "outputId": "76d4c995-4423-478b-9c66-01b48a27c866"
   },
   "outputs": [],
   "source": [
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "from typing import Dict, Union, Optional\n",
    "\n",
    "class KnowledgeBase(BaseModel):\n",
    "    ## Fields of the BaseModel, which will be validated/assigned when the knowledge base is constructed\n",
    "    topic: str = Field('general', description=\"Current conversation topic\")\n",
    "    user_preferences: Dict[str, Union[str, int]] = Field({}, description=\"User preferences and choices\")\n",
    "    session_notes: str = Field(\"\", description=\"Notes on the ongoing session\")\n",
    "    unresolved_queries: list = Field([], description=\"Unresolved user queries\")\n",
    "    action_items: list = Field([], description=\"Actionable items identified during the conversation\")\n",
    "\n",
    "print(repr(KnowledgeBase(topic = \"Travel\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_g8qYKZBwYsO",
   "metadata": {
    "id": "_g8qYKZBwYsO"
   },
   "source": [
    "<br>\n",
    "\n",
    "The true strength of this approach lies in the additional LLM-centric functionalities provided by LangChain which we can integrate for our use-cases. One such feature is the `PydanticOutputParser` which enhances the Pydantic objects with capabilities like automatic format instruction generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "H6uK93zwwdcn",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 146,
     "status": "ok",
     "timestamp": 1703083424311,
     "user": {
      "displayName": "Vadim Kudlay",
      "userId": "00553664172613290122"
     },
     "user_tz": 360
    },
    "id": "H6uK93zwwdcn",
    "outputId": "fb4f1a54-0a3c-46e6-fe19-4f1f9282bc61"
   },
   "outputs": [],
   "source": [
    "from langchain.output_parsers import PydanticOutputParser\n",
    "\n",
    "instruct_string = PydanticOutputParser(pydantic_object=KnowledgeBase).get_format_instructions()\n",
    "pprint(instruct_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oFyxUZEMwp9s",
   "metadata": {
    "id": "oFyxUZEMwp9s"
   },
   "source": [
    "This functionality generates instructions for creating valid inputs to the Knowledge Base, which in turn helps the LLM by providing a concrete one-shot example of the desired output format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JIGIXNfmwqAz",
   "metadata": {
    "id": "JIGIXNfmwqAz"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### **Runnable Extraction Module**\n",
    "\n",
    "Knowing that we have this Pydantic object which can be used to generate good LLM instructions, we can make a Runnable that wraps the functionality of our Pydantic class and streamlines the prompting, generating, and updating of the knowledge base:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Zya1sucNwwIO",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3469,
     "status": "ok",
     "timestamp": 1703086115894,
     "user": {
      "displayName": "Vadim Kudlay",
      "userId": "00553664172613290122"
     },
     "user_tz": 360
    },
    "id": "Zya1sucNwwIO",
    "outputId": "fb5a0b21-dcfc-4e6f-f621-a5466047fc3f"
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "## Definition of RExtract\n",
    "def RExtract(pydantic_class, llm, prompt):\n",
    "    '''\n",
    "    Runnable Extraction module\n",
    "    Returns a knowledge dictionary populated by slot-filling extraction\n",
    "    '''\n",
    "    parser = PydanticOutputParser(pydantic_object=pydantic_class)\n",
    "    instruct_merge = RunnableAssign({'format_instructions' : lambda x: parser.get_format_instructions()})\n",
    "    def preparse(string):\n",
    "        if '{' not in string: string = '{' + string\n",
    "        if '}' not in string: string = string + '}'\n",
    "        string = (string\n",
    "            .replace(\"\\\\_\", \"_\")\n",
    "            .replace(\"\\n\", \" \")\n",
    "            .replace(\"\\]\", \"]\")\n",
    "            .replace(\"\\[\", \"[\")\n",
    "        )\n",
    "        # print(string)  ## Good for diagnostics\n",
    "        return string\n",
    "    return instruct_merge | prompt | llm | preparse | parser\n",
    "\n",
    "################################################################################\n",
    "## Practical Use of RExtract\n",
    "\n",
    "parser_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Update the knowledge base: {format_instructions}. Only use information from the input.\"\n",
    "    \"\\n\\nNEW MESSAGE: {input}\"\n",
    ")\n",
    "\n",
    "extractor = RExtract(KnowledgeBase, instruct_llm, parser_prompt)\n",
    "\n",
    "knowledge = extractor.invoke({'input' : \"I love flowers so much! The orchids are amazing! Can you buy me some?\"})\n",
    "pprint(knowledge)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6Rjncqne2-zm",
   "metadata": {
    "id": "6Rjncqne2-zm"
   },
   "source": [
    "<br>\n",
    "\n",
    "Do keep in mind that this process can fail due to the fuzzy nature of LLM prediction, especially with models that are not optimized for instruction-following! For this process, it's important to have a strong instruction-following LLM with extra checks and graceful failure routines. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qePT8nsdDskr",
   "metadata": {
    "id": "qePT8nsdDskr"
   },
   "source": [
    "<br>\n",
    "\n",
    "#### **Dynamic Knowledge Base Updates**\n",
    "\n",
    "Finally, we can create a system that continually updates the Knowledge Base throughout the conversation. This is done by feeding the current state of the Knowledge Base, along with new user inputs, back into the system for ongoing updates.\n",
    "\n",
    "The following is an example system that shows off both the formulation's power of filling details as well as the limitations of assuming that filling performance will be as good as general response performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8yKtrLeBDu35",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6433,
     "status": "ok",
     "timestamp": 1703083433913,
     "user": {
      "displayName": "Vadim Kudlay",
      "userId": "00553664172613290122"
     },
     "user_tz": 360
    },
    "id": "8yKtrLeBDu35",
    "outputId": "282c71b5-8056-4115-d4c1-70254a1e9caf"
   },
   "outputs": [],
   "source": [
    "class KnowledgeBase(BaseModel):\n",
    "    firstname: str = Field('unknown', description=\"Chatting user's first name, unknown if unknown\")\n",
    "    lastname: str = Field('unknown', description=\"Chatting user's last name, unknown if unknown\")\n",
    "    location: str = Field('unknown', description=\"Where the user is located\")\n",
    "    summary: str = Field('unknown', description=\"Running summary of conversation. Update this with new input\")\n",
    "    response: str = Field('unknown', description=\"An ideal response to the user based on their new message\")\n",
    "\n",
    "\n",
    "parser_prompt = ChatPromptTemplate.from_template(\n",
    "    \"You are chatting with a user. The user just responded ('input'). Please update the knowledge base.\"\n",
    "    \" Record your response in the 'response' tag to continue the conversation.\"\n",
    "    \" Do not hallucinate any details, and make sure the knowledge base is not redundant.\"\n",
    "    \" Update the entries frequently to adapt to the conversation flow.\"\n",
    "    \"\\n{format_instructions}\"\n",
    "    \"\\n\\nOLD KNOWLEDGE BASE: {know_base}\"\n",
    "    \"\\n\\nNEW MESSAGE: {input}\"\n",
    "    \"\\n\\nNEW KNOWLEDGE BASE:\"\n",
    ")\n",
    "\n",
    "## Switch to a more powerful base model\n",
    "instruct_llm = ChatNVIDIA(model=\"mistralai/mixtral-8x22b-instruct-v0.1\") | StrOutputParser()\n",
    "\n",
    "extractor = RExtract(KnowledgeBase, instruct_llm, parser_prompt)\n",
    "info_update = RunnableAssign({'know_base' : extractor})\n",
    "\n",
    "## Initialize the knowledge base and see what you get\n",
    "state = {'know_base' : KnowledgeBase()}\n",
    "state['input'] = \"My name is Carmen Sandiego! Guess where I am! Hint: It's somewhere in the United States.\"\n",
    "state = info_update.invoke(state)\n",
    "pprint(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0415ceac-bd40-4bc6-a3a1-c66f863c258e",
   "metadata": {},
   "outputs": [],
   "source": [
    "state['input'] = \"I'm in a place considered the birthplace of Jazz.\"\n",
    "state = info_update.invoke(state)\n",
    "pprint(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e0a259-3695-47ea-8e35-9288e2f28f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "state['input'] = \"Yeah, I'm in New Orleans... How did you know?\"\n",
    "state = info_update.invoke(state)\n",
    "pprint(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Abnm3hvCtQaF",
   "metadata": {
    "id": "Abnm3hvCtQaF"
   },
   "source": [
    "<br>\n",
    "\n",
    "This example demonstrates how a running state chain can be effectively utilized to manage a conversation with evolving context and requirements, making it a powerful tool for developing sophisticated interactive systems.\n",
    "\n",
    "The next sections of this notebook will expand on these concepts by exploring two specific applications: **Document Knowledge Bases** and **Database-Querying Chatbots**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa26368-6580-4a69-8abf-3fc3a6460070",
   "metadata": {
    "id": "7fa26368-6580-4a69-8abf-3fc3a6460070"
   },
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 4: [Exercise]** Airline Customer Service Bot\n",
    "\n",
    "In this exercise, we can expand on the tools we've learned about to implement a simple but effective dialog manager chatbot. For this exercise, we will make an airline support bot that wants to help a client find out about their flight!\n",
    "\n",
    "Let's create a simple database-like interface to get some customer information from a dictionary!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4965800-e71c-4d56-a16e-c8bc4ea77d55",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 382,
     "status": "ok",
     "timestamp": 1703094239296,
     "user": {
      "displayName": "Vadim Kudlay",
      "userId": "00553664172613290122"
     },
     "user_tz": 360
    },
    "id": "e4965800-e71c-4d56-a16e-c8bc4ea77d55",
    "outputId": "c68c924b-cef0-4d6f-a461-8d7d4a22a904"
   },
   "outputs": [],
   "source": [
    "#######################################################################################\n",
    "## Function that can be queried for information. Implementation details not important\n",
    "def get_flight_info(d: dict) -> str:\n",
    "    \"\"\"\n",
    "    Example of a retrieval function which takes a dictionary as key. Resembles SQL DB Query\n",
    "    \"\"\"\n",
    "    req_keys = ['first_name', 'last_name', 'confirmation']\n",
    "    assert all((key in d) for key in req_keys), f\"Expected dictionary with keys {req_keys}, got {d}\"\n",
    "\n",
    "    ## Static dataset. get_key and get_val can be used to work with it, and db is your variable\n",
    "    keys = req_keys + [\"departure\", \"destination\", \"departure_time\", \"arrival_time\", \"flight_day\"]\n",
    "    values = [\n",
    "        [\"Jane\", \"Doe\", 12345, \"San Jose\", \"New Orleans\", \"12:30 PM\", \"9:30 PM\", \"tomorrow\"],\n",
    "        [\"John\", \"Smith\", 54321, \"New York\", \"Los Angeles\", \"8:00 AM\", \"11:00 AM\", \"Sunday\"],\n",
    "        [\"Alice\", \"Johnson\", 98765, \"Chicago\", \"Miami\", \"7:00 PM\", \"11:00 PM\", \"next week\"],\n",
    "        [\"Bob\", \"Brown\", 56789, \"Dallas\", \"Seattle\", \"1:00 PM\", \"4:00 PM\", \"yesterday\"],\n",
    "    ]\n",
    "    get_key = lambda d: \"|\".join([d['first_name'], d['last_name'], str(d['confirmation'])])\n",
    "    get_val = lambda l: {k:v for k,v in zip(keys, l)}\n",
    "    db = {get_key(get_val(entry)) : get_val(entry) for entry in values}\n",
    "\n",
    "    # Search for the matching entry\n",
    "    data = db.get(get_key(d))\n",
    "    if not data:\n",
    "        return (\n",
    "            f\"Based on {req_keys} = {get_key(d)}) from your knowledge base, no info on the user flight was found.\"\n",
    "            \" This process happens every time new info is learned. If it's important, ask them to confirm this info.\"\n",
    "        )\n",
    "    return (\n",
    "        f\"{data['first_name']} {data['last_name']}'s flight from {data['departure']} to {data['destination']}\"\n",
    "        f\" departs at {data['departure_time']} {data['flight_day']} and lands at {data['arrival_time']}.\"\n",
    "    )\n",
    "\n",
    "#######################################################################################\n",
    "## Usage example. Actually important\n",
    "\n",
    "print(get_flight_info({\"first_name\" : \"Jane\", \"last_name\" : \"Doe\", \"confirmation\" : 12345}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0897722-c517-4b43-834c-0604810d5416",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_flight_info({\"first_name\" : \"Alice\", \"last_name\" : \"Johnson\", \"confirmation\" : 98765}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847eb9a2-535d-4a9b-9b75-e698a349b87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_flight_info({\"first_name\" : \"Bob\", \"last_name\" : \"Brown\", \"confirmation\" : 27494}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0ac572-4f4c-49b7-abc5-d716c49a4523",
   "metadata": {
    "id": "7d0ac572-4f4c-49b7-abc5-d716c49a4523"
   },
   "source": [
    "<br>\n",
    "\n",
    "This is a really good interface to bring up because it can reasonably serve two purposes:\n",
    "- It can be used to provide up-to-date information from an external environment (a database) regarding a user's situation.\n",
    "- It can also be used as a hard gating mechanism to prevent unauthorized disclosure of sensitive information (since that would be very bad).\n",
    "\n",
    "If our network had access to this kind of interface, it would be able to query for and retrieve this information on a user's behalf! For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6799f182-a9b2-4078-a5c7-c53f9a8ef9a2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "executionInfo": {
     "elapsed": 2258,
     "status": "ok",
     "timestamp": 1703083436156,
     "user": {
      "displayName": "Vadim Kudlay",
      "userId": "00553664172613290122"
     },
     "user_tz": 360
    },
    "id": "6799f182-a9b2-4078-a5c7-c53f9a8ef9a2",
    "outputId": "7591a21d-8eac-4dd6-f001-d953e12af547"
   },
   "outputs": [],
   "source": [
    "external_prompt = ChatPromptTemplate.from_template(\n",
    "    \"You are a SkyFlow chatbot, and you are helping a customer with their issue.\"\n",
    "    \" Please help them with their question, remembering that your job is to represent SkyFlow airlines.\"\n",
    "    \" Assume SkyFlow uses industry-average practices regarding arrival times, operations, etc.\"\n",
    "    \" (This is a trade secret. Do not disclose).\"  ## soft reinforcement\n",
    "    \" Please keep your discussion short and sweet if possible. Avoid saying hello unless necessary.\"\n",
    "    \" The following is some context that may be useful in answering the question.\"\n",
    "    \"\\n\\nContext: {context}\"\n",
    "    \"\\n\\nUser: {input}\"\n",
    ")\n",
    "\n",
    "basic_chain = external_prompt | instruct_llm\n",
    "\n",
    "basic_chain.invoke({\n",
    "    'input' : 'Can you please tell me when I need to get to the airport?',\n",
    "    'context' : get_flight_info({\"first_name\" : \"Jane\", \"last_name\" : \"Doe\", \"confirmation\" : 12345}),\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d7288b-d931-4d3f-ba78-4bc0c5883fa0",
   "metadata": {
    "id": "18d7288b-d931-4d3f-ba78-4bc0c5883fa0"
   },
   "source": [
    "<br>\n",
    "\n",
    "This is interesting enough, but how do we actually get this system working in the wild? It turns out that we can use the KnowledgeBase formulation from above to supply this kind of information like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d078210a-41ab-453a-8934-ac234fb0ed6f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 312,
     "status": "ok",
     "timestamp": 1703086127938,
     "user": {
      "displayName": "Vadim Kudlay",
      "userId": "00553664172613290122"
     },
     "user_tz": 360
    },
    "id": "d078210a-41ab-453a-8934-ac234fb0ed6f",
    "outputId": "5a38a98a-8875-4d7b-ff89-04c7bbc14a69"
   },
   "outputs": [],
   "source": [
    "from langchain.pydantic_v1 import BaseModel, Field\n",
    "from typing import Dict, Union\n",
    "\n",
    "class KnowledgeBase(BaseModel):\n",
    "    first_name: str = Field('unknown', description=\"Chatting user's first name, `unknown` if unknown\")\n",
    "    last_name: str = Field('unknown', description=\"Chatting user's last name, `unknown` if unknown\")\n",
    "    confirmation: int = Field(-1, description=\"Flight Confirmation Number, `-1` if unknown\")\n",
    "    discussion_summary: str = Field(\"\", description=\"Summary of discussion so far, including locations, issues, etc.\")\n",
    "    open_problems: list = Field([], description=\"Topics that have not been resolved yet\")\n",
    "    current_goals: list = Field([], description=\"Current goal for the agent to address\")\n",
    "\n",
    "def get_key_fn(base: BaseModel) -> dict:\n",
    "    '''Given a dictionary with a knowledge base, return a key for get_flight_info'''\n",
    "    return {  ## More automatic options possible, but this is more explicit\n",
    "        'first_name' : base.first_name,\n",
    "        'last_name' : base.last_name,\n",
    "        'confirmation' : base.confirmation,\n",
    "    }\n",
    "\n",
    "know_base = KnowledgeBase(first_name = \"Jane\", last_name = \"Doe\", confirmation = 12345)\n",
    "\n",
    "# get_flight_info(get_key_fn(know_base))\n",
    "\n",
    "get_key = RunnableLambda(get_key_fn)\n",
    "(get_key | get_flight_info).invoke(know_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20c0c43-c031-4cb9-be48-a1eb4d85d289",
   "metadata": {
    "id": "c20c0c43-c031-4cb9-be48-a1eb4d85d289"
   },
   "source": [
    "<br>\n",
    "\n",
    "### **Objective:**\n",
    "\n",
    "You want a user to be able to invoke the following function call organically as part of a dialog exchange:\n",
    "\n",
    "```python\n",
    "get_flight_info({\"first_name\" : \"Jane\", \"last_name\" : \"Doe\", \"confirmation\" : 12345}) ->\n",
    "    \"Jane Doe's flight from San Jose to New Orleans departs at 12:30 PM tomorrow and lands at 9:30 PM.\"\n",
    "```\n",
    "\n",
    "`RExtract` is provided such that the following knowledge base syntax can be used:\n",
    "```python\n",
    "known_info = KnowledgeBase()\n",
    "extractor = RExtract(KnowledgeBase, InstructLLM(), parser_prompt)\n",
    "results = extractor.invoke({'info_base' : known_info, 'input' : 'My message'})\n",
    "known_info = results['info_base']\n",
    "```\n",
    "\n",
    "**Design a chatbot that implements the following features:**\n",
    "- The bot should start off by making small-talk, possibly helping the user with non-sensitive queries which don't require any private info access.\n",
    "- When the user starts to ask about things that are database-walled (both practically and legally), tell the user that they need to provide the relevant information.\n",
    "- When the retrieval is successful, the agent will be able to talk about the database-walled information.\n",
    "\n",
    "**This can be done with a variety of techniques, including the following:**\n",
    "- **Prompt Engineering and Context Parsing**, where the overall chat prompt stays roughly the same but the context is manipulated to to change agent behavior. For example, a failed db retrieval could be changed into an injection of natural-language instructions for how to resolve the problem such as *`\"Information could not be retrieved with keys {...}. Please ask the user for clarification or help them with known information.\"`*\n",
    "- **\"Prompt Passing,\"** where the active prompts are passed around as state variables and can be overridden by monitoring chains.\n",
    "- **Branching chains** such as [**`RunnableBranch`**](https://api.python.langchain.com/en/latest/core/runnables/langchain_core.runnables.branch.RunnableBranch.html) or more custom solutions that implement an conditional routing mechanism.\n",
    "    - In the case of [`RunnableBranch`](https://api.python.langchain.com/en/latest/core/runnables/langchain_core.runnables.branch.RunnableBranch.html), a `switch` syntax of the style:\n",
    "        ```python\n",
    "        from langchain.schema.runnable import RunnableBranch\n",
    "        RunnableBranch(\n",
    "            ((lambda x: 1 in x), RPrint(\"Has 1 (didn't check 2): \")),\n",
    "            ((lambda x: 2 in x), RPrint(\"Has 2 (not 1 though): \")),\n",
    "            RPrint(\"Has neither 1 not 2: \")\n",
    "        ).invoke([2, 1, 3]);  ## -> Has 1 (didn't check 2): [2, 1, 3]\n",
    "        ```\n",
    "\n",
    "Some prompts and a gradio loop are provided that might help with the effort, but the agent will currently just hallucinate! Please implement the internal chain to try and retrieve the relevant information. Before trying to implement, look over the default behavior of the model and note how it might hallucinate or forget things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c065dff4-ff23-4fcb-85fb-34f508f0b620",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 655
    },
    "id": "c065dff4-ff23-4fcb-85fb-34f508f0b620",
    "outputId": "67f46fc6-0469-443d-cd88-8a2758d2c29d"
   },
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import (\n",
    "    RunnableBranch,\n",
    "    RunnableLambda,\n",
    "    RunnableMap,       ## Wrap an implicit \"dictionary\" runnable\n",
    "    RunnablePassthrough,\n",
    ")\n",
    "from langchain.schema.runnable.passthrough import RunnableAssign\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import BaseMessage, SystemMessage, ChatMessage, AIMessage\n",
    "from typing import Iterable\n",
    "import gradio as gr\n",
    "\n",
    "external_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", (\n",
    "        \"You are a chatbot for SkyFlow Airlines, and you are helping a customer with their issue.\"\n",
    "        \" Please chat with them! Stay concise and clear!\"\n",
    "        \" Your running knowledge base is: {know_base}.\"\n",
    "        \" This is for you only; Do not mention it!\"\n",
    "        \" \\nUsing that, we retrieved the following: {context}\\n\"\n",
    "        \" If they provide info and the retrieval fails, ask to confirm their first/last name and confirmation.\"\n",
    "        \" Do not ask them any other personal info.\"\n",
    "        \" If it's not important to know about their flight, do not ask.\"\n",
    "        \" The checking happens automatically; you cannot check manually.\"\n",
    "    )),\n",
    "    (\"assistant\", \"{output}\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "])\n",
    "\n",
    "##########################################################################\n",
    "## Knowledge Base Things\n",
    "\n",
    "class KnowledgeBase(BaseModel):\n",
    "    first_name: str = Field('unknown', description=\"Chatting user's first name, `unknown` if unknown\")\n",
    "    last_name: str = Field('unknown', description=\"Chatting user's last name, `unknown` if unknown\")\n",
    "    confirmation: Optional[int] = Field(None, description=\"Flight Confirmation Number, `-1` if unknown\")\n",
    "    discussion_summary: str = Field(\"\", description=\"Summary of discussion so far, including locations, issues, etc.\")\n",
    "    open_problems: str = Field(\"\", description=\"Topics that have not been resolved yet\")\n",
    "    current_goals: str = Field(\"\", description=\"Current goal for the agent to address\")\n",
    "\n",
    "parser_prompt = ChatPromptTemplate.from_template(\n",
    "    \"You are a chat assistant representing the airline SkyFlow, and are trying to track info about the conversation.\"\n",
    "    \" You have just received a message from the user. Please fill in the schema based on the chat.\"\n",
    "    \"\\n\\n{format_instructions}\"\n",
    "    \"\\n\\nOLD KNOWLEDGE BASE: {know_base}\"\n",
    "    \"\\n\\nASSISTANT RESPONSE: {output}\"\n",
    "    \"\\n\\nUSER MESSAGE: {input}\"\n",
    "    \"\\n\\nNEW KNOWLEDGE BASE: \"\n",
    ")\n",
    "\n",
    "## Your goal is to invoke the following through natural conversation\n",
    "# get_flight_info({\"first_name\" : \"Jane\", \"last_name\" : \"Doe\", \"confirmation\" : 12345}) ->\n",
    "#     \"Jane Doe's flight from San Jose to New Orleans departs at 12:30 PM tomorrow and lands at 9:30 PM.\"\n",
    "\n",
    "chat_llm = ChatNVIDIA(model=\"meta/llama3-70b-instruct\") | StrOutputParser()\n",
    "instruct_llm = ChatNVIDIA(model=\"mistralai/mixtral-8x22b-instruct-v0.1\") | StrOutputParser()\n",
    "\n",
    "external_chain = external_prompt | chat_llm\n",
    "\n",
    "#####################################################################################\n",
    "## START TODO: Define the extractor and internal chain to satisfy the objective\n",
    "\n",
    "## TODO: Make a chain that will populate your knowledge base based on provided context\n",
    "knowbase_getter = lambda x: KnowledgeBase()\n",
    "\n",
    "## TODO: Make a chain to pull d[\"know_base\"] and outputs a retrieval from db\n",
    "database_getter = lambda x: \"Not implemented\"\n",
    "\n",
    "## These components integrate to make your internal chain\n",
    "internal_chain = (\n",
    "    RunnableAssign({'know_base' : knowbase_getter})\n",
    "    | RunnableAssign({'context' : database_getter})\n",
    ")\n",
    "\n",
    "## END TODO\n",
    "#####################################################################################\n",
    "\n",
    "state = {'know_base' : KnowledgeBase()}\n",
    "\n",
    "def chat_gen(message, history=[], return_buffer=True):\n",
    "\n",
    "    ## Pulling in, updating, and printing the state\n",
    "    global state\n",
    "    state['input'] = message\n",
    "    state['history'] = history\n",
    "    state['output'] = \"\" if not history else history[-1][1]\n",
    "\n",
    "    ## Generating the new state from the internal chain\n",
    "    state = internal_chain.invoke(state)\n",
    "    print(\"State after chain run:\")\n",
    "    pprint({k:v for k,v in state.items() if k != \"history\"})\n",
    "    \n",
    "    ## Streaming the results\n",
    "    buffer = \"\"\n",
    "    for token in external_chain.stream(state):\n",
    "        buffer += token\n",
    "        yield buffer if return_buffer else token\n",
    "\n",
    "def queue_fake_streaming_gradio(chat_stream, history = [], max_questions=8):\n",
    "\n",
    "    ## Mimic of the gradio initialization routine, where a set of starter messages can be printed off\n",
    "    for human_msg, agent_msg in history:\n",
    "        if human_msg: print(\"\\n[ Human ]:\", human_msg)\n",
    "        if agent_msg: print(\"\\n[ Agent ]:\", agent_msg)\n",
    "\n",
    "    ## Mimic of the gradio loop with an initial message from the agent.\n",
    "    for _ in range(max_questions):\n",
    "        message = input(\"\\n[ Human ]: \")\n",
    "        print(\"\\n[ Agent ]: \")\n",
    "        history_entry = [message, \"\"]\n",
    "        for token in chat_stream(message, history, return_buffer=False):\n",
    "            print(token, end='')\n",
    "            history_entry[1] += token\n",
    "        history += [history_entry]\n",
    "        print(\"\\n\")\n",
    "\n",
    "## history is of format [[User response 0, Bot response 0], ...]\n",
    "chat_history = [[None, \"Hello! I'm your SkyFlow agent! How can I help you?\"]]\n",
    "\n",
    "## Simulating the queueing of a streaming gradio interface, using python input\n",
    "queue_fake_streaming_gradio(\n",
    "    chat_stream = chat_gen,\n",
    "    history = chat_history\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783b27e0-55ed-4c0e-8af3-ebdc331e1f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# state = {'know_base' : KnowledgeBase()}\n",
    "\n",
    "# chatbot = gr.Chatbot(value=[[None, \"Hello! I'm your SkyFlow agent! How can I help you?\"]])\n",
    "# demo = gr.ChatInterface(chat_gen, chatbot=chatbot).queue().launch(debug=True, share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbfc194-ce36-4983-82a9-b195eb1faaef",
   "metadata": {
    "id": "3cbfc194-ce36-4983-82a9-b195eb1faaef"
   },
   "source": [
    "<br>\n",
    "\n",
    "----\n",
    "\n",
    "<br>\n",
    "\n",
    "**NOTE:**\n",
    "- You may need to explicitly hit the STOP button and try to relaunch your gradio interface if it hangs up after an exception. This is a known Jupyter Notebook environment issue which should not be experienced in dedicated Gradio-running files.\n",
    "- **Your chat directive is duplicated here for quick access:**\n",
    "```python\n",
    "## Your goal is to invoke the following through natural conversation\n",
    "get_flight_info({\n",
    "    \"first_name\" : \"Jane\",\n",
    "    \"last_name\" : \"Doe\",\n",
    "    \"confirmation\" : 12345,\n",
    "}) -> \"Jane Doe's flight from San Jose to New Orleans departs at 12:30 PM tomorrow and lands at 9:30 PM.\"\n",
    "```\n",
    "- **To confirm that your system works, you could try the following dialog or something similar:**\n",
    "```\n",
    "> How's it going?\n",
    "> Can you tell me a bit about skyflow?\n",
    "> Can you tell me about my flight?\n",
    "> My name is Jane Doe and my flight confirmation is 12345\n",
    "> Can you tell me when I should get to my flight?\n",
    "```\n",
    "- **Solutions To Exercises Can Be Found In The Solutions Directory.** This is the first exercise with a noted solution, and additional exercises from the future notebooks will be found there."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7PZooExdzgLh",
   "metadata": {
    "id": "7PZooExdzgLh"
   },
   "source": [
    "-----\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Part 5:** Wrap-Up\n",
    "\n",
    "The goal of this notebook was to introduce some more advanced LangChain material revolving around the use of knowledge bases and running state chains! The exercise here was pretty involved, so congrats on finishing it!\n",
    "\n",
    "### <font color=\"#76b900\">**Great Job!**</font>\n",
    "\n",
    "### **Next Steps:**\n",
    "1. **[Optional]** Revisit the **\"Questions To Think About\" Section** at the top of the notebook and think about some possible answers.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b02f5e-6977-4c6a-b47c-7abb1d592ce4",
   "metadata": {
    "id": "b1b02f5e-6977-4c6a-b47c-7abb1d592ce4"
   },
   "source": [
    "<center><a href=\"https://www.nvidia.com/en-us/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
